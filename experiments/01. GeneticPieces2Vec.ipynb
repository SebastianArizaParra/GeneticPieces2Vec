{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c50851d-9560-4aa8-bf66-b9f66600c386",
   "metadata": {},
   "source": [
    "# GeneticPieces2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020823c-fb58-48cf-88e4-b1f86c05b525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:14:29.752087Z",
     "iopub.status.busy": "2025-05-01T23:14:29.751761Z",
     "iopub.status.idle": "2025-05-01T23:14:33.517781Z",
     "shell.execute_reply": "2025-05-01T23:14:33.517345Z",
     "shell.execute_reply.started": "2025-05-01T23:14:29.752067Z"
    }
   },
   "outputs": [],
   "source": [
    "#Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#DL\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "#Sytem\n",
    "from pymongo import MongoClient\n",
    "import sys\n",
    "#Tokenizers\n",
    "import sentencepiece as spm\n",
    "#Graphic\n",
    "import matplotlib.pyplot as plt\n",
    "#Custom\n",
    "sys.path.append('utils/')\n",
    "import DataGenerator as dg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddc6e1",
   "metadata": {},
   "source": [
    "## ðŸ“ Define Paths and Database Parameters\n",
    "\n",
    "We define variables for:\n",
    "\n",
    "- MongoDB database and collection names.\n",
    "- BPE tokenizer model.\n",
    "- A csv with the required train, tune, test partition IDs.\n",
    "- Paths to save the model output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b1dd2-b7e8-4031-bd28-84d60f2da60a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:14:33.518685Z",
     "iopub.status.busy": "2025-05-01T23:14:33.518443Z",
     "iopub.status.idle": "2025-05-01T23:14:33.521198Z",
     "shell.execute_reply": "2025-05-01T23:14:33.520818Z",
     "shell.execute_reply.started": "2025-05-01T23:14:33.518667Z"
    }
   },
   "outputs": [],
   "source": [
    "db_name = \"------\"\n",
    "collection_name = \"------\"\n",
    "tokenizer_model_path = \"------\"\n",
    "partitions_path = \"------\"\n",
    "model_path = \"------\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d6799-eb17-4a7b-a335-593aa289a5a8",
   "metadata": {},
   "source": [
    "## Load organisms ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa174d-68f4-4718-a4b5-7e3e61da7a75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:14:34.440834Z",
     "iopub.status.busy": "2025-05-01T23:14:34.440407Z",
     "iopub.status.idle": "2025-05-01T23:14:34.449687Z",
     "shell.execute_reply": "2025-05-01T23:14:34.449319Z",
     "shell.execute_reply.started": "2025-05-01T23:14:34.440816Z"
    }
   },
   "outputs": [],
   "source": [
    "partitions = pd.read_csv(partitions_path)\n",
    "training_IDs = list(partitions.loc[partitions['partition']=='Train', 'ID'])\n",
    "tuning_IDs = list(partitions.loc[partitions['partition']=='Tune', 'ID'])\n",
    "testing_IDs = list(partitions.loc[partitions['partition']=='Test', 'ID'])\n",
    "\n",
    "print(f' Training: {len(training_IDs)} \\n Tune: {len(tuning_IDs)} \\n Test: {len(testing_IDs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a35ba-cd6f-4746-a2b4-8a756f5bec71",
   "metadata": {},
   "source": [
    "## Build model pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd73b54",
   "metadata": {},
   "source": [
    "This custom Keras layer computes the dot product between two embedding tensors using Einstein summation.\n",
    "It is used to calculate similarity between target and context embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d0438-d3be-4e20-80cb-afb7212d13a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:14:37.278006Z",
     "iopub.status.busy": "2025-05-01T23:14:37.277662Z",
     "iopub.status.idle": "2025-05-01T23:14:37.280697Z",
     "shell.execute_reply": "2025-05-01T23:14:37.280237Z",
     "shell.execute_reply.started": "2025-05-01T23:14:37.277990Z"
    }
   },
   "outputs": [],
   "source": [
    "class dotlayer(keras.layers.Layer):\n",
    "    def __init__(self , **kwargs):\n",
    "        super(dotlayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x1, x2):\n",
    "        return keras.ops.einsum('bfc,bec->be', x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed05316-3153-4c3e-bc61-150923d3a9fb",
   "metadata": {},
   "source": [
    "## Build an Hypermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0beed",
   "metadata": {},
   "source": [
    "## HyperModel Definition for Word2Vec Training\n",
    "Defines a KerasTuner `HyperModel` class to build and train a Skip-gram Word2Vec model.\n",
    "Includes support for hyperparameter tuning of:\n",
    "- Embedding size\n",
    "- Learning rate\n",
    "- L2 regularization lambda\n",
    "- Context window size\n",
    "- Number of negative samples\n",
    "\n",
    "Uses a custom `DataGenerator` to stream training and validation data from MongoDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75a11a-e987-4308-b23b-728f93f8a0f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:14:38.023966Z",
     "iopub.status.busy": "2025-05-01T23:14:38.023745Z",
     "iopub.status.idle": "2025-05-01T23:14:38.030644Z",
     "shell.execute_reply": "2025-05-01T23:14:38.030251Z",
     "shell.execute_reply.started": "2025-05-01T23:14:38.023950Z"
    }
   },
   "outputs": [],
   "source": [
    "class W2VHyperModel(kt.HyperModel):\n",
    "    def __init__(self, train_ids, val_ids): \n",
    "        keras.backend.clear_session(free_memory=True)\n",
    "        self.train_ids = train_ids\n",
    "        self.val_ids = val_ids\n",
    "\n",
    "    def build(self, hp):\n",
    "        embedding_size = hp.Int('embedding size', min_value=32,max_value=512,step=2,sampling=\"log\")\n",
    "        lr = hp.Float(\"learning rate\",min_value=1e-3,max_value=1e-1,step=5,sampling=\"log\")\n",
    "        reg = hp.Float(\"lambda\",min_value=1e-5,max_value=1e-3,step=10,sampling=\"log\")\n",
    "        context_size = hp.Choice('context size', values=[5,9,13,17,21,25])\n",
    "        negative_samples = hp.Choice('negative samples', values=[3,5,7])\n",
    "\n",
    "        self.train_gen = dg.DataGenerator(db_name, collection_name, organism_IDs = self.train_ids, tokenizer_path = tokenizer_model_path, shuffle = True, batch_size = 100, \n",
    "                                                     context_size=context_size, negative_samples=negative_samples, vocab_size=12000, max_pair=1000)\n",
    "        self.val_gen = dg.DataGenerator(db_name, collection_name, organism_IDs = self.val_ids, tokenizer_path = tokenizer_model_path, shuffle = False, batch_size = 100, \n",
    "                                                   context_size=context_size, negative_samples=negative_samples, vocab_size=12000, max_pair=100)\n",
    "\n",
    "        embedding_model = keras.models.Sequential()\n",
    "        embedding_model.add(keras.layers.Input(shape=(None,)))\n",
    "        embedding_model.add(keras.layers.Embedding(12000, embedding_size, embeddings_regularizer = keras.regularizers.L2(reg)))\n",
    "        embedding_model.add(keras.layers.BatchNormalization())\n",
    "            \n",
    "        target_input = keras.layers.Input(shape=(1,))\n",
    "        context_input = keras.layers.Input(shape=(None,))\n",
    "        target_emb = embedding_model(target_input)\n",
    "        context_emb = embedding_model(context_input)\n",
    "        out = keras.activations.softmax(dotlayer()(target_emb, context_emb))\n",
    "\n",
    "        model = keras.Model(inputs=[target_input, context_input], outputs=out)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr),loss=keras.losses.CategoricalCrossentropy(from_logits=False),metrics=['AUC'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, epochs=1, callbacks=None, **kwargs):\n",
    "                \n",
    "        return model.fit(self.train_gen,validation_data=self.val_gen,epochs=epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e692f-795a-48f2-8f02-acdc2f615d7f",
   "metadata": {},
   "source": [
    "### Training example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6c48b",
   "metadata": {},
   "source": [
    "## Train the Model and Visualize Performance\n",
    "Trains the model using early stopping and plots:\n",
    "- Categorical Cross-Entropy Loss over epochs\n",
    "- AUC (Area Under the Curve) metric over epochs\n",
    "\n",
    "These metrics help evaluate model performance and overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b2f0a3-4bfc-4a70-a343-d4217e9ee77f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T01:20:48.055005Z",
     "iopub.status.busy": "2025-04-28T01:20:48.054621Z",
     "iopub.status.idle": "2025-04-28T01:20:48.987206Z",
     "shell.execute_reply": "2025-04-28T01:20:48.986837Z",
     "shell.execute_reply.started": "2025-04-28T01:20:48.054989Z"
    }
   },
   "outputs": [],
   "source": [
    "hp = kt.HyperParameters()\n",
    "hm = W2VHyperModel(training_IDs, tuning_IDs)\n",
    "w2vmodel = hm.build(hp)\n",
    "w2vmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc1c79-ee11-4a94-9a72-8afa7977c2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T01:20:48.988138Z",
     "iopub.status.busy": "2025-04-28T01:20:48.987802Z",
     "iopub.status.idle": "2025-04-28T01:20:49.067880Z",
     "shell.execute_reply": "2025-04-28T01:20:49.067393Z",
     "shell.execute_reply.started": "2025-04-28T01:20:48.988115Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(w2vmodel, show_shapes=True, to_file=f'{model_path}/W2V structure.png',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492daba-44fb-4601-a4af-36d569960495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T01:20:51.960942Z",
     "iopub.status.busy": "2025-04-28T01:20:51.960737Z",
     "iopub.status.idle": "2025-04-28T01:23:37.815095Z",
     "shell.execute_reply": "2025-04-28T01:23:37.814673Z",
     "shell.execute_reply.started": "2025-04-28T01:20:51.960925Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='min',restore_best_weights=True)\n",
    "train_history = hm.fit(hp, w2vmodel, epochs=1, callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd1a19-1585-4fe4-a118-7a0b1a948b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T00:14:01.251410Z",
     "iopub.status.busy": "2025-04-28T00:14:01.251069Z",
     "iopub.status.idle": "2025-04-28T00:14:01.460939Z",
     "shell.execute_reply": "2025-04-28T00:14:01.460467Z",
     "shell.execute_reply.started": "2025-04-28T00:14:01.251390Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4)) # 1 fila, 2 columnas\n",
    "\n",
    "ax1.plot(train_history.history['loss'], color='blue', label = 'Train')\n",
    "ax1.plot(train_history.history['val_loss'], color='red', label = 'Test')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('loss (categorical cross entropy)')\n",
    "ax1.set_title('Loss during training')\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_history.history['AUC'], color='blue', label = 'Train')\n",
    "ax2.plot(train_history.history['val_AUC'], color='red', label = 'Test')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('AUC (ROC)')\n",
    "ax2.set_title('AUC during training')\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8952dc-e3ae-49aa-a07d-9e0a265b3d97",
   "metadata": {},
   "source": [
    "## Hyperparameter search\n",
    "\n",
    "Runing on tmux session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c86ca0",
   "metadata": {},
   "source": [
    "Performs Random Search using `keras_tuner` to find optimal hyperparameters for the Word2Vec model.\n",
    "Key configurations:\n",
    "- `objective`: validation AUC\n",
    "- `max_trials`: number of different hyperparameter combinations\n",
    "- `executions_per_trial`: repeat each combination for stability\n",
    "- Uses EarlyStopping and TensorBoard for monitoring\n",
    "\n",
    "This script is designed to be run in a background tmux session for long experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc6a15-020b-40ff-9290-7d76b5e92a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T01:24:03.405125Z",
     "iopub.status.busy": "2025-04-28T01:24:03.404695Z",
     "iopub.status.idle": "2025-04-28T01:24:03.409578Z",
     "shell.execute_reply": "2025-04-28T01:24:03.409126Z",
     "shell.execute_reply.started": "2025-04-28T01:24:03.405106Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/HP_search.py\n",
    "\n",
    "############################################################################################################################## Imports and configurations #########################################################################################################################################\n",
    "\n",
    "#Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#DL\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "#Sytem\n",
    "from pymongo import MongoClient\n",
    "import sys\n",
    "#Tokenizers\n",
    "import sentencepiece as spm\n",
    "#Graphic\n",
    "import matplotlib.pyplot as plt\n",
    "#Custom\n",
    "sys.path.append(\"------\")\n",
    "import DataGenerator as dg\n",
    "\n",
    "############################################################################################################################## Important paths #####################################################################################################################################################\n",
    "\n",
    "db_name = \"------\"\n",
    "collection_name = \"------\"\n",
    "tokenizer_model_path = \"------\"\n",
    "partitions_path = \"------\"\n",
    "model_path = \"------\"\n",
    "\n",
    "############################################################################################################################## Load data ###########################################################################################################################################################\n",
    "\n",
    "partitions = pd.read_csv(partitions_path)\n",
    "training_IDs = list(partitions.loc[partitions['partition']=='Train', 'ID'])\n",
    "tuning_IDs = list(partitions.loc[partitions['partition']=='Tune', 'ID'])\n",
    "testing_IDs = list(partitions.loc[partitions['partition']=='Test', 'ID'])\n",
    "\n",
    "print(f' Training: {len(training_IDs)} \\n Tune: {len(tuning_IDs)} \\n Test: {len(testing_IDs)}')\n",
    "\n",
    "############################################################################################################################## Model builder ########################################################################################################################################################\n",
    "\n",
    "class dotlayer(keras.layers.Layer):\n",
    "    def __init__(self , **kwargs):\n",
    "        super(dotlayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x1, x2):\n",
    "        return keras.ops.einsum('bfc,bec->be', x1, x2)\n",
    "\n",
    "class W2VHyperModel(kt.HyperModel):\n",
    "    def __init__(self, train_ids, val_ids):\n",
    "\n",
    "        keras.backend.clear_session(free_memory=True)\n",
    "        self.train_ids = train_ids\n",
    "        self.val_ids = val_ids\n",
    "\n",
    "    def build(self, hp):\n",
    "        embedding_size = hp.Int('embedding size', min_value=32,max_value=512,step=2,sampling=\"log\")\n",
    "        lr = hp.Float(\"learning rate\",min_value=1e-3,max_value=1e-1,step=5,sampling=\"log\")\n",
    "        reg = hp.Float(\"lambda\",min_value=1e-5,max_value=1e-3,step=10,sampling=\"log\")\n",
    "        context_size = hp.Choice('context size', values=[5,9,13,17,21,25])\n",
    "        negative_samples = hp.Choice('negative samples', values=[3,5,7])\n",
    "\n",
    "        self.train_gen = dg.DataGenerator(db_name, collection_name, organism_IDs = self.train_ids, tokenizer_path = tokenizer_model_path, shuffle = True, batch_size = 100, \n",
    "                                                     context_size=context_size, negative_samples=negative_samples, vocab_size=12000, max_pair=1000)\n",
    "        self.val_gen = dg.DataGenerator(db_name, collection_name, organism_IDs = self.val_ids, tokenizer_path = tokenizer_model_path, shuffle = False, batch_size = 100, \n",
    "                                                   context_size=context_size, negative_samples=negative_samples, vocab_size=12000, max_pair=100)\n",
    "\n",
    "        embedding_model = keras.models.Sequential()\n",
    "        embedding_model.add(keras.layers.Input(shape=(None,)))\n",
    "        embedding_model.add(keras.layers.Embedding(12000, embedding_size, embeddings_regularizer = keras.regularizers.L2(reg)))\n",
    "        embedding_model.add(keras.layers.BatchNormalization())\n",
    "            \n",
    "        target_input = keras.layers.Input(shape=(1,))\n",
    "        context_input = keras.layers.Input(shape=(None,))\n",
    "        target_emb = embedding_model(target_input)\n",
    "        context_emb = embedding_model(context_input)\n",
    "        out = keras.activations.softmax(dotlayer()(target_emb, context_emb))\n",
    "\n",
    "        model = keras.Model(inputs=[target_input, context_input], outputs=out)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr),loss=keras.losses.CategoricalCrossentropy(from_logits=False),metrics=['AUC'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, epochs=1, callbacks=None, **kwargs):\n",
    "                \n",
    "        return model.fit(self.train_gen,validation_data=self.val_gen,epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "############################################################################################################################## Hyper paramter search ################################################################################################################################################\n",
    "\n",
    "# Callbacks\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='min',restore_best_weights=True)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(f\"{model_path}/HP_search/TensorBoard\")\n",
    "#Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "        hypermodel=W2VHyperModel(training_IDs, tuning_IDs),\n",
    "        objective=\"val_AUC\",\n",
    "        max_trials=20,\n",
    "        executions_per_trial=3,\n",
    "        overwrite=True,\n",
    "        directory=f\"{model_path}/HP_search\",\n",
    "        project_name=\"Genetic Word2Vec\")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "#Start search\n",
    "tuner.search(epochs=20, callbacks=[early_stopping_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0b92f-a7de-4306-bde2-d15d18a1335d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T02:58:45.245261Z",
     "iopub.status.busy": "2025-04-21T02:58:45.244876Z",
     "iopub.status.idle": "2025-04-21T02:58:45.247244Z",
     "shell.execute_reply": "2025-04-21T02:58:45.246853Z",
     "shell.execute_reply.started": "2025-04-21T02:58:45.245244Z"
    }
   },
   "source": [
    "## Main train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11d4eb",
   "metadata": {},
   "source": [
    "Loads the best hyperparameters found in the tuning stage.\n",
    "Trains the final model on the full training set using early stopping and saves:\n",
    "- Full model architecture and weights in `.keras` format\n",
    "- Weights separately in `.h5` format\n",
    "\n",
    "This final model is used for downstream genomic analysis or phenotype prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38ef7b-cc8f-48e8-b975-d7ce40d96ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:20:58.308624Z",
     "iopub.status.busy": "2025-05-01T23:20:58.308367Z",
     "iopub.status.idle": "2025-05-01T23:20:58.466552Z",
     "shell.execute_reply": "2025-05-01T23:20:58.466132Z",
     "shell.execute_reply.started": "2025-05-01T23:20:58.308608Z"
    }
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "        hypermodel=W2VHyperModel(training_IDs, tuning_IDs),\n",
    "        objective=\"val_AUC\",\n",
    "        max_trials=20,\n",
    "        executions_per_trial=3,\n",
    "        overwrite=False,\n",
    "        directory=f\"{model_path}/HP_search\",\n",
    "        project_name=\"Genetic Word2Vec\")\n",
    "\n",
    "tuner.results_summary(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32de1c7-e649-4ac4-88fa-10ab801044b5",
   "metadata": {},
   "source": [
    "run on tmux session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c5faa-72bb-4ed3-9b20-5b0883ca2f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:22:17.738234Z",
     "iopub.status.busy": "2025-05-01T23:22:17.737960Z",
     "iopub.status.idle": "2025-05-01T23:22:17.742791Z",
     "shell.execute_reply": "2025-05-01T23:22:17.742335Z",
     "shell.execute_reply.started": "2025-05-01T23:22:17.738217Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/final_train.py\n",
    "############################################################################################################################## Imports and configurations #########################################################################################################################################\n",
    "\n",
    "#Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#DL\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "#Sytem\n",
    "from pymongo import MongoClient\n",
    "import sys\n",
    "#Tokenizers\n",
    "import sentencepiece as spm\n",
    "#Graphic\n",
    "import matplotlib.pyplot as plt\n",
    "#Custom\n",
    "sys.path.append('\"------\"')\n",
    "import DataGenerator as dg\n",
    "\n",
    "############################################################################################################################## Important paths #####################################################################################################################################################\n",
    "\n",
    "db_name = \"------\"\n",
    "collection_name = \"------\"\n",
    "tokenizer_model_path = \"------\"\n",
    "partitions_path = \"------\"\n",
    "model_path = \"------\"\n",
    "\n",
    "############################################################################################################################## Load data ###########################################################################################################################################################\n",
    "\n",
    "partitions = pd.read_csv(partitions_path)\n",
    "training_IDs = list(partitions.loc[partitions['partition']=='Train', 'ID'])\n",
    "tuning_IDs = list(partitions.loc[partitions['partition']=='Tune', 'ID'])\n",
    "testing_IDs = list(partitions.loc[partitions['partition']=='Test', 'ID'])\n",
    "\n",
    "print(f' Training: {len(training_IDs)} \\n Tune: {len(tuning_IDs)} \\n Test: {len(testing_IDs)}')\n",
    "\n",
    "############################################################################################################################## Model builder ########################################################################################################################################################\n",
    "\n",
    "class dotlayer(keras.layers.Layer):\n",
    "    def __init__(self , **kwargs):\n",
    "        super(dotlayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x1, x2):\n",
    "        return keras.ops.einsum('bfc,bec->be', x1, x2)\n",
    "\n",
    "class W2VHyperModel(kt.HyperModel):\n",
    "    def __init__(self, train_ids, val_ids):\n",
    "\n",
    "        keras.backend.clear_session(free_memory=True)\n",
    "        self.train_ids = train_ids\n",
    "        self.val_ids = val_ids\n",
    "\n",
    "    def build(self, hp):\n",
    "        embedding_size = hp.Int('embedding size', min_value=32,max_value=512,step=2,sampling=\"log\")\n",
    "        lr = hp.Float(\"learning rate\",min_value=1e-3,max_value=1e-1,step=5,sampling=\"log\")\n",
    "        reg = hp.Float(\"lambda\",min_value=1e-5,max_value=1e-3,step=10,sampling=\"log\")\n",
    "        context_size = hp.Choice('context size', values=[5,9,13,17,21,25])\n",
    "        negative_samples = hp.Choice('negative samples', values=[3,5,7])\n",
    "\n",
    "        self.train_gen = dg.DataGenerator(db_name, collection_name, organism_IDs = self.train_ids, tokenizer_path = tokenizer_model_path, shuffle = True, batch_size = 100, \n",
    "                                                     context_size=context_size, negative_samples=negative_samples, vocab_size=12000, max_pair=1000)\n",
    "        self.val_gen = dg.DataGenerator(db_name, collection_name, organism_IDs = self.val_ids, tokenizer_path = tokenizer_model_path, shuffle = False, batch_size = 100, \n",
    "                                                   context_size=context_size, negative_samples=negative_samples, vocab_size=12000, max_pair=100)\n",
    "\n",
    "        embedding_model = keras.models.Sequential()\n",
    "        embedding_model.add(keras.layers.Input(shape=(None,)))\n",
    "        embedding_model.add(keras.layers.Embedding(12000, embedding_size, embeddings_regularizer = keras.regularizers.L2(reg)))\n",
    "        embedding_model.add(keras.layers.BatchNormalization())\n",
    "            \n",
    "        target_input = keras.layers.Input(shape=(1,))\n",
    "        context_input = keras.layers.Input(shape=(None,))\n",
    "        target_emb = embedding_model(target_input)\n",
    "        context_emb = embedding_model(context_input)\n",
    "        out = keras.activations.softmax(dotlayer()(target_emb, context_emb))\n",
    "\n",
    "        model = keras.Model(inputs=[target_input, context_input], outputs=out)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr),loss=keras.losses.CategoricalCrossentropy(from_logits=False),metrics=['AUC'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, epochs=1, callbacks=None, **kwargs):\n",
    "                \n",
    "        return model.fit(self.train_gen,validation_data=self.val_gen,epochs=epochs, callbacks=callbacks)\n",
    "############################################################################################################################## Final train ################################################################################################################################################\n",
    "# Load tuner\n",
    "tuner = kt.RandomSearch(\n",
    "        hypermodel=W2VHyperModel(training_IDs, tuning_IDs),\n",
    "        objective=\"val_AUC\",\n",
    "        max_trials=20,\n",
    "        executions_per_trial=3,\n",
    "        overwrite=False,\n",
    "        directory=f\"{model_path}/HP_search\",\n",
    "        project_name=\"Genetic Word2Vec\")\n",
    "# Callbacks\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='min',restore_best_weights=True, min_delta=0.01)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(f\"{model_path}/final model/TensorBoard\")\n",
    "# Build model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "hm = W2VHyperModel(training_IDs, tuning_IDs)\n",
    "w2vmodel = hm.build(best_hp)\n",
    "keras.utils.plot_model(w2vmodel, show_shapes=True, to_file=f'{model_path}/final model/W2V structure.png')\n",
    "# Start train\n",
    "hm.fit(best_hp, w2vmodel, epochs=1_000_000, callbacks=[early_stopping_callback, tensorboard_callback])\n",
    "w2vmodel.save(f\"{model_path}/final model/W2Vstructure.keras\")\n",
    "w2vmodel.save_weights(f\"{model_path}/final model/W2V.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd7afd-820b-403f-a7cc-ffe29d72aeef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
