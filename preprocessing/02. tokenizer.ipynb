{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff451e33-f627-4814-abcb-dc364f5231c1",
   "metadata": {},
   "source": [
    "# BPE tokenization on genetic sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7220f1f",
   "metadata": {},
   "source": [
    "This notebook applies subword tokenization (BPE) using SentencePiece on genomic haplotype sequences extracted from a database. This step prepares the data for training models like Word2Vec or Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5363ef9-a852-4a48-a4ea-c544229d424b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:16:22.009080Z",
     "iopub.status.busy": "2025-04-27T23:16:22.008743Z",
     "iopub.status.idle": "2025-04-27T23:16:22.462212Z",
     "shell.execute_reply": "2025-04-27T23:16:22.461823Z",
     "shell.execute_reply.started": "2025-04-27T23:16:22.009055Z"
    }
   },
   "outputs": [],
   "source": [
    "#numeric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#system\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "#graphic\n",
    "import matplotlib.pyplot as plt\n",
    "#tokenizers\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ac98f",
   "metadata": {},
   "source": [
    "## üìÅ Define Paths and Database Parameters\n",
    "\n",
    "We define variables for:\n",
    "\n",
    "- MongoDB database and collection names.\n",
    "- File path to the dataset partition CSV (train/test splits).\n",
    "- Paths to save the tokenizer input file and the trained tokenizer model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a16cb8-22d1-4470-8008-1b1c2f57aa74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:16:22.463005Z",
     "iopub.status.busy": "2025-04-27T23:16:22.462819Z",
     "iopub.status.idle": "2025-04-27T23:16:22.465397Z",
     "shell.execute_reply": "2025-04-27T23:16:22.464939Z",
     "shell.execute_reply.started": "2025-04-27T23:16:22.462991Z"
    }
   },
   "outputs": [],
   "source": [
    "db_name = \"------\"\n",
    "collection_name = \"------\"\n",
    "dset_partition_path = \"------\"\n",
    "tokenizer_file_path = \"------\"\n",
    "tokenizer_model_path = \"------\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f262903-e95a-4b5c-9968-8fecdc63ef34",
   "metadata": {},
   "source": [
    "## Get training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3f893-e1e0-4f6e-bbd3-1c063f942fec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:16:23.268293Z",
     "iopub.status.busy": "2025-04-27T23:16:23.267859Z",
     "iopub.status.idle": "2025-04-27T23:16:23.275082Z",
     "shell.execute_reply": "2025-04-27T23:16:23.274379Z",
     "shell.execute_reply.started": "2025-04-27T23:16:23.268268Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:11111/\")\n",
    "\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1e66a",
   "metadata": {},
   "source": [
    "## Extract Sequences for Tokenizer Training\n",
    "\n",
    "Here we retrieve haplotype sequences from the MongoDB collection to prepare training data for the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741c213-43a4-45f1-90fa-8d0d8cfd1474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:16:23.706721Z",
     "iopub.status.busy": "2025-04-27T23:16:23.706288Z",
     "iopub.status.idle": "2025-04-27T23:16:23.711732Z",
     "shell.execute_reply": "2025-04-27T23:16:23.711275Z",
     "shell.execute_reply.started": "2025-04-27T23:16:23.706703Z"
    }
   },
   "outputs": [],
   "source": [
    "query = {\"organism_ID\":'1'}\n",
    "n_sequences = collection.count_documents(query)\n",
    "training_sequences = collection.find(query)\n",
    "print(f'Number of traning sequences: {n_sequences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e4521-61ca-46c9-98d1-10d13941aa56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:16:24.348229Z",
     "iopub.status.busy": "2025-04-27T23:16:24.347978Z",
     "iopub.status.idle": "2025-04-27T23:16:24.680385Z",
     "shell.execute_reply": "2025-04-27T23:16:24.679903Z",
     "shell.execute_reply.started": "2025-04-27T23:16:24.348210Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open(tokenizer_file_path, \"w\")\n",
    "file.close()\n",
    "\n",
    "for i, sequence in enumerate(training_sequences):\n",
    "    file = open(tokenizer_file_path, \"a\")\n",
    "    haplotype_1 = sequence['haplotype_1']\n",
    "    file.write(haplotype_1.upper() + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    print(f'{i+1} of {n_sequences}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c86895-d279-43b4-9540-5d8257e498af",
   "metadata": {},
   "source": [
    "## Train BPE with sentence piece tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42be57a",
   "metadata": {},
   "source": [
    "We train a subword tokenizer using the SentencePiece library with the following settings:\n",
    "\n",
    "- `--input`: path to the text file containing haplotype sequences.\n",
    "- `--model_prefix`: prefix for the output model and vocabulary files.\n",
    "- `--vocab_size=12000`: the desired vocabulary size.\n",
    "- `--model_type=bpe`: specifies Byte Pair Encoding (BPE) as the tokenization algorithm.\n",
    "- `--unk_id=0`, `--unk_piece=N`: unknown tokens are assigned ID 0 and represented as `'N'`.\n",
    "- `--num_threads=1`: sets single-threaded training (can be increased for faster training).\n",
    "- `--minloglevel=2`: suppresses warnings and errors shown.\n",
    "\n",
    "This model will later be used to tokenize sequences into subword units for downstream tasks such as sequence modeling or embedding training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57865e-2b63-4fdc-bf32-a4a3b1eda8b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:22:57.245762Z",
     "iopub.status.busy": "2025-04-27T23:22:57.245488Z",
     "iopub.status.idle": "2025-04-27T23:23:08.844616Z",
     "shell.execute_reply": "2025-04-27T23:23:08.844025Z",
     "shell.execute_reply.started": "2025-04-27T23:22:57.245744Z"
    }
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(f'--input={tokenizer_file_path} --model_prefix={tokenizer_model_path} --vocab_size=12000 --model_type=bpe --unk_id=0 --unk_piece=N --num_threads=1 --minloglevel=2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0473272",
   "metadata": {},
   "source": [
    "## üß™ Load Tokenizer and Test on Example Sequence\n",
    "\n",
    "This section demonstrates how to use the trained SentencePiece tokenizer:\n",
    "\n",
    "1. We load the BPE model using `SentencePieceProcessor()`.\n",
    "2. We retrieve a single example sequence from the MongoDB collection.\n",
    "3. We print the original `haplotype_1` in uppercase.\n",
    "4. We tokenize the sequence:\n",
    "   - `encode_as_pieces`: returns the tokenized sequence as subword strings.\n",
    "   - `encode_as_ids`: returns the tokenized sequence as corresponding token IDs.\n",
    "\n",
    "This test verifies that the tokenizer was trained successfully and produces expected subword units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae824e-b7c0-4330-8675-786b5413861e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:23:11.711130Z",
     "iopub.status.busy": "2025-04-27T23:23:11.710706Z",
     "iopub.status.idle": "2025-04-27T23:23:11.723177Z",
     "shell.execute_reply": "2025-04-27T23:23:11.722795Z",
     "shell.execute_reply.started": "2025-04-27T23:23:11.711111Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load(f'{tokenizer_model_path}.model')\n",
    "\n",
    "xample_sequence = collection.find_one()\n",
    "haplotype_1 = example_sequence['haplotype_1'].upper()\n",
    "print(haplotype_1)\n",
    "print('-'*100)\n",
    "print(tokenizer.encode_as_pieces(haplotype_1))\n",
    "print('-'*100)\n",
    "print(tokenizer.encode_as_ids(haplotype_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
