{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff451e33-f627-4814-abcb-dc364f5231c1",
   "metadata": {},
   "source": [
    "# Preprocessing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5363ef9-a852-4a48-a4ea-c544229d424b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:03:31.284958Z",
     "iopub.status.busy": "2025-04-27T23:03:31.284638Z",
     "iopub.status.idle": "2025-04-27T23:03:31.287282Z",
     "shell.execute_reply": "2025-04-27T23:03:31.286924Z",
     "shell.execute_reply.started": "2025-04-27T23:03:31.284941Z"
    }
   },
   "outputs": [],
   "source": [
    "#Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#DL\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#Sytem\n",
    "from pymongo import MongoClient\n",
    "#Tokenizers\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e36b02",
   "metadata": {},
   "source": [
    "## üìÅ Define Paths and Database Parameters\n",
    "\n",
    "We define variables for:\n",
    "\n",
    "- MongoDB database and collection names.\n",
    "- BPE tokenizer model.\n",
    "- A csv with the required train, tune, test partition IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a16cb8-22d1-4470-8008-1b1c2f57aa74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:03:31.699647Z",
     "iopub.status.busy": "2025-04-27T23:03:31.699279Z",
     "iopub.status.idle": "2025-04-27T23:03:31.702002Z",
     "shell.execute_reply": "2025-04-27T23:03:31.701574Z",
     "shell.execute_reply.started": "2025-04-27T23:03:31.699630Z"
    }
   },
   "outputs": [],
   "source": [
    "db_name = \"------\"\n",
    "collection_name = \"------\"\n",
    "tokenizer_model_path = \"------\"\n",
    "partitions_path = \"------\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924de11f-b067-4610-84f3-f05962cf3283",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95deac4d",
   "metadata": {},
   "source": [
    "This section shows:\n",
    "\n",
    "- Implementing a custom `DataGenerator` class that inherits from `keras.utils.Sequence` to efficiently load and preprocess data in batches for model training.\n",
    "- Instantiating the data generator for the training set and iterating through batches to generate input-output pairs.\n",
    "- Saving the `DataGenerator` class as a utility Python file for reuse.\n",
    "\n",
    "This approach enables scalable and reproducible data preprocessing for downstream machine learning tasks.}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8ec8b",
   "metadata": {},
   "source": [
    "DataGenerator class to yield training batches for embedding models with the following features:\n",
    "\n",
    "- **`db_name`**: Name of the MongoDB database holding genotype data.  \n",
    "- **`collection_name`**: Name of the collection with gene sequence records.  \n",
    "- **`organism_IDs`**: List of organism IDs sampled randomly per batch to pair with gene IDs.  \n",
    "- **`tokenizer_path`**: Path prefix to the SentencePiece tokenizer model (without `.model` extension).  \n",
    "- **`shuffle`**: Boolean flag to shuffle gene IDs at the end of each epoch to improve training randomness.  \n",
    "- **`batch_size`**: Number of gene samples processed per batch.  \n",
    "- **`context_size`**: Size of the sliding window used to extract context words around an anchor word in sequences.  \n",
    "- **`negative_samples`**: Number of negative context samples generated per positive anchor-context pair.  \n",
    "- **`vocab_size`**: Total vocabulary size used to sample negative examples uniformly at random.  \n",
    "- **`max_pair`**: Maximum number of anchor-context pairs randomly selected from each gene's sequence per batch.\n",
    "\n",
    "Key methods and behaviors:\n",
    "\n",
    "- **`__len__`**: Calculates how many batches fit in one epoch given dataset size and batch size.  \n",
    "- **`__getitem__`**: Retrieves a batch of data by fetching gene sequences from MongoDB, tokenizing, and generating training pairs.  \n",
    "- **`on_epoch_end`**: Shuffles gene IDs if enabled, to ensure different ordering across epochs.  \n",
    "- **`__data_generation`**:  \n",
    "  - Randomly pairs each gene with an organism ID for batch diversity.  \n",
    "  - Queries the database for haplotype sequences of these pairs.  \n",
    "  - Tokenizes haplotype sequences using BPE previously model.  \n",
    "  - Splits sequences into fixed-length context windows.  \n",
    "  - Extracts anchor words (middle token of each context window) and corresponding context words.  \n",
    "  - Randomly samples a subset of these pairs up to `max_pair`.  \n",
    "  - Generates negative samples by sampling random tokens from the vocabulary.  \n",
    "  - Combines positive and negative samples to form the final input (`X`) and label (`Y`) batches.\n",
    "\n",
    "This class is tailored to efficiently produce training data for GeneticPieces2Vec Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f643b-6a15-4a28-83e5-17661524daab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T01:20:20.974323Z",
     "iopub.status.busy": "2025-04-28T01:20:20.974199Z",
     "iopub.status.idle": "2025-04-28T01:20:21.125454Z",
     "shell.execute_reply": "2025-04-28T01:20:21.124806Z",
     "shell.execute_reply.started": "2025-04-28T01:20:20.974308Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, db_name, collection_name, organism_IDs, tokenizer_path, shuffle, batch_size, context_size, negative_samples, vocab_size, max_pair, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.organism_IDs = organism_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.context_size = context_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_pair = max_pair\n",
    "\n",
    "        self.tokenizer = spm.SentencePieceProcessor()\n",
    "        self.tokenizer.load(f'{tokenizer_path}.model')\n",
    "\n",
    "        self.client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "        self.db = self.client[db_name]\n",
    "        self.collection = self.db[collection_name]\n",
    "        self.genes_IDs = self.collection.distinct('gene_ID')\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.genes_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        batch_genes_IDS = self.genes_IDs[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, Y = self.__data_generation(batch_genes_IDS)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.genes_IDs)\n",
    "\n",
    "    def __data_generation(self, IDs):\n",
    "        organisms = list(map(str,np.random.choice(self.organism_IDs, size = self.batch_size)))\n",
    "        query = {\"$or\": [\n",
    "                    {'gene_ID': gene, 'organism_ID': organism}\n",
    "                    for gene,organism in zip(IDs, organisms)\n",
    "                   ]}\n",
    "        genes = self.collection.find(query)\n",
    "        \n",
    "        total_pairs = np.zeros([1,2 + self.negative_samples])\n",
    "        for i,gene in enumerate(genes):\n",
    "            haplotype_1, haplotype_2 = gene['haplotype_1'].upper(), gene['haplotype_2'].upper()\n",
    "            seq_1, seq_2 = np.array(self.tokenizer.encode_as_ids(haplotype_1)), np.array(self.tokenizer.encode_as_ids(haplotype_2))\n",
    "\n",
    "            seq_len = np.min([len(seq_1), len(seq_2)])\n",
    "            n_contexts = np.floor(seq_len/self.context_size)\n",
    "            seq_1, seq_2 = seq_1[:int(n_contexts*self.context_size)], seq_2[:int(n_contexts*self.context_size)]\n",
    "            seq_1, seq_2 = seq_1.reshape([int(n_contexts), self.context_size]), seq_2.reshape([int(n_contexts), self.context_size])\n",
    "            seq = np.concatenate((seq_1, seq_2), axis=0)\n",
    "\n",
    "            anchor_words = seq[:, int(self.context_size/2)]\n",
    "            anchor_words = np.repeat(anchor_words, self.context_size-1)\n",
    "            context_words = np.delete(seq, int(self.context_size/2), axis=1).flatten()\n",
    "            pairs = np.column_stack((anchor_words, context_words))\n",
    "            \n",
    "            reduced_pairs = pairs[np.random.randint(0, high=len(pairs), size=(self.max_pair)),:]\n",
    "            \n",
    "            negative_context = np.random.randint(1, high=self.vocab_size, size=(len(reduced_pairs), self.negative_samples))\n",
    "            gene_pairs = np.column_stack((reduced_pairs, negative_context))\n",
    "            \n",
    "            total_pairs = np.concatenate((total_pairs,gene_pairs), axis=0)\n",
    "            \n",
    "        total_pairs = total_pairs[1:]\n",
    "        X = (total_pairs[:,0], total_pairs[:,1:])\n",
    "        Y = np.zeros([len(X[0]), 1 + self.negative_samples])\n",
    "        Y[:,0] = 1\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c2852-2c0e-455c-bc51-ccc60beb85ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:09:41.369455Z",
     "iopub.status.busy": "2025-04-27T23:09:41.369098Z",
     "iopub.status.idle": "2025-04-27T23:09:41.373825Z",
     "shell.execute_reply": "2025-04-27T23:09:41.373386Z",
     "shell.execute_reply.started": "2025-04-27T23:09:41.369436Z"
    }
   },
   "outputs": [],
   "source": [
    "partitions = pd.read_csv(partitions_path)\n",
    "training_IDs = list(partitions.loc[partitions['partition']=='Train', 'ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dfaf0-203a-4b6d-95bf-bcd8d2d59101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:09:47.808085Z",
     "iopub.status.busy": "2025-04-27T23:09:47.807647Z",
     "iopub.status.idle": "2025-04-27T23:09:47.825191Z",
     "shell.execute_reply": "2025-04-27T23:09:47.824740Z",
     "shell.execute_reply.started": "2025-04-27T23:09:47.808066Z"
    }
   },
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(db_name, collection_name, organism_IDs = training_IDs, tokenizer_path = tokenizer_model_path, shuffle = False, batch_size = 32, context_size=5, negative_samples=5, vocab_size=9000, max_pair=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3080c-1d85-4dad-bb3b-e669673bb3e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T23:09:49.401373Z",
     "iopub.status.busy": "2025-04-27T23:09:49.401120Z",
     "iopub.status.idle": "2025-04-27T23:11:34.752654Z",
     "shell.execute_reply": "2025-04-27T23:11:34.752217Z",
     "shell.execute_reply.started": "2025-04-27T23:09:49.401356Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(train_gen.__len__()):\n",
    "    X, Y = train_gen.__getitem__(i)\n",
    "    print(f'{i+1} of {train_gen.__len__()}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b62dae-dc08-4c2d-a26b-bdabf994cb1f",
   "metadata": {},
   "source": [
    "## Save data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e3ae4-d284-484b-a03a-33863dc05c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T01:20:32.430490Z",
     "iopub.status.busy": "2025-04-28T01:20:32.430176Z",
     "iopub.status.idle": "2025-04-28T01:20:32.434457Z",
     "shell.execute_reply": "2025-04-28T01:20:32.434086Z",
     "shell.execute_reply.started": "2025-04-28T01:20:32.430472Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /home/jmalagont/Documentos/GWord2Vec/algorithms/utils/DataGenerator.py\n",
    "\n",
    "#Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#DL\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#Sytem\n",
    "from pymongo import MongoClient\n",
    "#Tokenizers\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, db_name, collection_name, organism_IDs, tokenizer_path, shuffle, batch_size, context_size, negative_samples, vocab_size, max_pair, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.organism_IDs = organism_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.context_size = context_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_pair = max_pair\n",
    "\n",
    "        self.tokenizer = spm.SentencePieceProcessor()\n",
    "        self.tokenizer.load(f'{tokenizer_path}.model')\n",
    "\n",
    "        self.client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "        self.db = self.client[db_name]\n",
    "        self.collection = self.db[collection_name]\n",
    "        self.genes_IDs = self.collection.distinct('gene_ID')\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.genes_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        batch_genes_IDS = self.genes_IDs[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, Y = self.__data_generation(batch_genes_IDS)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.genes_IDs)\n",
    "\n",
    "    def __data_generation(self, IDs):\n",
    "        organisms = list(map(str,np.random.choice(self.organism_IDs, size = self.batch_size)))\n",
    "        query = {\"$or\": [\n",
    "                    {'gene_ID': gene, 'organism_ID': organism}\n",
    "                    for gene,organism in zip(IDs, organisms)\n",
    "                   ]}\n",
    "        genes = self.collection.find(query)\n",
    "        \n",
    "        total_pairs = np.zeros([1,2 + self.negative_samples])\n",
    "        for i,gene in enumerate(genes):\n",
    "            haplotype_1, haplotype_2 = gene['haplotype_1'].upper(), gene['haplotype_2'].upper()\n",
    "            seq_1, seq_2 = np.array(self.tokenizer.encode_as_ids(haplotype_1)), np.array(self.tokenizer.encode_as_ids(haplotype_2))\n",
    "\n",
    "            seq_len = np.min([len(seq_1), len(seq_2)])\n",
    "            n_contexts = np.floor(seq_len/self.context_size)\n",
    "            seq_1, seq_2 = seq_1[:int(n_contexts*self.context_size)], seq_2[:int(n_contexts*self.context_size)]\n",
    "            seq_1, seq_2 = seq_1.reshape([int(n_contexts), self.context_size]), seq_2.reshape([int(n_contexts), self.context_size])\n",
    "            seq = np.concatenate((seq_1, seq_2), axis=0)\n",
    "\n",
    "            anchor_words = seq[:, int(self.context_size/2)]\n",
    "            anchor_words = np.repeat(anchor_words, self.context_size-1)\n",
    "            context_words = np.delete(seq, int(self.context_size/2), axis=1).flatten()\n",
    "            pairs = np.column_stack((anchor_words, context_words))\n",
    "            \n",
    "            reduced_pairs = pairs[np.random.randint(0, high=len(pairs), size=(self.max_pair)),:]\n",
    "            \n",
    "            negative_context = np.random.randint(1, high=self.vocab_size, size=(len(reduced_pairs), self.negative_samples))\n",
    "            gene_pairs = np.column_stack((reduced_pairs, negative_context))\n",
    "            \n",
    "            total_pairs = np.concatenate((total_pairs,gene_pairs), axis=0)\n",
    "            \n",
    "        total_pairs = total_pairs[1:]\n",
    "        X = (total_pairs[:,0], total_pairs[:,1:])\n",
    "        Y = np.zeros([len(X[0]), 1 + self.negative_samples])\n",
    "        Y[:,0] = 1\n",
    "        return X, Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
